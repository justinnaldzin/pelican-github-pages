{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Spark SQL Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using Apache Spark v2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "from random import randint\n",
    "import json\n",
    "import logging\n",
    "import pandas\n",
    "from inflection import underscore\n",
    "from datetime import datetime, timezone\n",
    "from threading import Thread\n",
    "from sqlalchemy import types\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Spark SQL Benchmarking\").master(\"local[*]\").config(\"spark.executor.memory\", \"2g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create database\n",
    "database_name = 'spark_sql_benchmarking'\n",
    "spark.sql('CREATE DATABASE IF NOT EXISTS ' + database_name)\n",
    "spark.catalog.setCurrentDatabase(database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# REFERENCE\n",
    "#sqlContext.tableNames('spark_sql_benchmarking')          # ['test_table_1']\n",
    "#sqlContext.tables('spark_sql_benchmarking').toPandas()   # returns a pandas.DataFrame\n",
    "##spark.catalog.listDatabases()  # [Database(name='default', description='default database', locationUri='file:/jupyter-vagrant/notebook/spark-warehouse'), Database(name='spark_sql_benchmarking', description='', locationUri='file:/jupyter-vagrant/notebook/spark-warehouse/spark_sql_benchmarking.db')]\n",
    "##spark.catalog.currentDatabase()  # 'spark_sql_benchmarking'\n",
    "##spark.catalog.listTables('spark_sql_benchmarking')  # [Table(name='test_table_1', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n",
    "\n",
    "#sqlContext.registerDataFrameAsTable(dataframe, 'df_registertable')\n",
    "#dataframe.createOrReplaceTempView('df_tempview')\n",
    "#dataframe.createGlobalTempView('df_globaltempview2')\n",
    "#sqlContext.createExternalTable('df_external')#, path=None, source=None, schema=None, **options)\n",
    "#sqlContext.createExternalTable(tableName, \"parquet\", someDF.schema, Map(\"path\" -> path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "#dataframe = spark.catalog.createExternalTable('temp2_external_csv', 'data/test_table_1.csv', 'csv', header='true', inferschema='true')\n",
    "#spark.table('temp2_external_csv').schema\n",
    "\n",
    "# Alternative syntax\n",
    "#sqlContext.sql('CREATE TABLE temp_external_csv USING CSV')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Configure logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "\n",
    "handler = logging.StreamHandler()\n",
    "handler.setLevel(logging.INFO)\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Timer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.end = time.perf_counter()\n",
    "        self.interval = self.end - self.start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Search for data files on local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-06 19:27:55,767 [INFO] Found the following files in path:  /jupyter-vagrant/notebook/data\n",
      "2017-03-06 19:27:55,770 [INFO] /jupyter-vagrant/notebook/data/test_table_1.csv\n",
      "2017-03-06 19:27:55,772 [INFO] /jupyter-vagrant/notebook/data/test_table_2.csv\n"
     ]
    }
   ],
   "source": [
    "# full directory path to where the data files are stored.  These will be used to create the tables and insert into database.\n",
    "data_path = os.path.join(os.path.abspath(os.curdir), 'data')\n",
    "\n",
    "data_filepath_list = [os.path.join(data_path, filename) for filename in os.listdir(data_path) if filename.endswith(\".csv\")]\n",
    "if not data_filepath_list:\n",
    "    logging.error(\"No data files found in path:  \" + str(data_path))\n",
    "    sys.exit(1)\n",
    "tables_dataframe = pandas.DataFrame({'table_name': [(os.path.splitext(os.path.basename(filename))[0])\n",
    "                                                    for filename in data_filepath_list]})\n",
    "logging.info('Found the following files in path:  ' + str(data_path))\n",
    "for filename in data_filepath_list:\n",
    "    logging.info(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load data into Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-06 19:27:58,232 [INFO] Reading data file: /jupyter-vagrant/notebook/data/test_table_1.csv\n",
      "2017-03-06 19:28:15,325 [INFO] 460949 rows loaded.\n",
      "2017-03-06 19:28:15,638 [INFO] Registering the csv source type as a SQL temporary view...\n",
      "2017-03-06 19:28:15,695 [INFO] Successfully created table: test_table_1\n",
      "2017-03-06 19:28:15,697 [INFO] Saving the DataFrame into alternative data source types:\n",
      "2017-03-06 19:28:15,698 [INFO] Saving test_table_1.parquet...\n",
      "2017-03-06 19:28:38,148 [INFO] Registering the parquet source type as a SQL temporary view...\n",
      "2017-03-06 19:28:38,505 [INFO] Successfully created table: test_table_1.parquet\n",
      "2017-03-06 19:28:38,507 [INFO] Saving test_table_1.json...\n",
      "2017-03-06 19:28:58,013 [INFO] Registering the json source type as a SQL temporary view...\n",
      "2017-03-06 19:29:10,832 [INFO] Successfully created table: test_table_1.json\n",
      "2017-03-06 19:29:10,852 [INFO] Saving test_table_1.orc...\n",
      "2017-03-06 19:29:37,160 [INFO] Registering the orc source type as a SQL temporary view...\n",
      "2017-03-06 19:29:37,373 [INFO] Successfully created table: test_table_1.orc\n",
      "2017-03-06 19:29:37,377 [INFO] Reading data file: /jupyter-vagrant/notebook/data/test_table_2.csv\n",
      "2017-03-06 19:29:37,829 [INFO] 120 rows loaded.\n",
      "2017-03-06 19:29:37,840 [INFO] Registering the csv source type as a SQL temporary view...\n",
      "2017-03-06 19:29:37,849 [INFO] Successfully created table: test_table_2\n",
      "2017-03-06 19:29:37,851 [INFO] Saving the DataFrame into alternative data source types:\n",
      "2017-03-06 19:29:37,852 [INFO] Saving test_table_2.parquet...\n",
      "2017-03-06 19:29:38,360 [INFO] Registering the parquet source type as a SQL temporary view...\n",
      "2017-03-06 19:29:38,525 [INFO] Successfully created table: test_table_2.parquet\n",
      "2017-03-06 19:29:38,527 [INFO] Saving test_table_2.json...\n",
      "2017-03-06 19:29:38,856 [INFO] Registering the json source type as a SQL temporary view...\n",
      "2017-03-06 19:29:39,134 [INFO] Successfully created table: test_table_2.json\n",
      "2017-03-06 19:29:39,136 [INFO] Saving test_table_2.orc...\n",
      "2017-03-06 19:29:39,640 [INFO] Registering the orc source type as a SQL temporary view...\n",
      "2017-03-06 19:29:40,287 [INFO] Successfully created table: test_table_2.orc\n"
     ]
    }
   ],
   "source": [
    "for filepath in data_filepath_list:\n",
    "    logging.info(\"Reading data file: \" + filepath)\n",
    "    table_name = os.path.splitext(os.path.basename(filepath))[0] #  set table name to basename of filepath\n",
    "    \n",
    "    # Create an external table from CSV\n",
    "    #spark.catalog.createExternalTable(table_name, path=filepath, source='csv', header='true', inferschema='true')\n",
    "\n",
    "    # The following is needed to rename columns or choose specific columns:\n",
    "    if True:\n",
    "        # Read CSV into Spark DataFrame\n",
    "        dataframe = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(filepath)\n",
    "        logging.info(str(dataframe.count()) + \" rows loaded.\")\n",
    "        #dataframe.dropna(axis=1, thresh=dataframe.count()//10, inplace=True)  # drop columns having over 90% NaN values\n",
    "        cols = [c for c in map(str.upper, map(underscore, dataframe.columns))]  # camelcase to underscore to uppercase\n",
    "        dataframe = dataframe.toDF(*cols)\n",
    "        #dataframe.cache()  # cache data in memory\n",
    "\n",
    "        # Create temporary view from CSV\n",
    "        logging.info(\"Registering the csv source type as a SQL temporary view...\")\n",
    "        dataframe.createOrReplaceTempView(table_name)\n",
    "        #spark.registerDataFrameAsTable(dataframe, table_name)  # alternative syntax; throws exception if table already exists\n",
    "        logging.info(\"Successfully created table: \" + table_name)\n",
    "    \n",
    "    # Save as alternative data source types and create temporary views\n",
    "    logging.info(\"Saving the DataFrame into alternative data source types:\")\n",
    "    data_source_types = ['parquet', 'json', 'orc']\n",
    "    for source_type in data_source_types:\n",
    "        table_path = table_name + \".\" + source_type\n",
    "        logging.info(\"Saving \" + table_path + \"...\")\n",
    "        dataframe.write.save(table_path, format=source_type, mode='overwrite')\n",
    "        logging.info(\"Registering the \" + source_type + \" source type as a SQL temporary view...\")            \n",
    "        temp_dataframe = getattr(spark.read, source_type)(table_path)\n",
    "        temp_dataframe.createOrReplaceTempView('`' + table_path + '`')\n",
    "        logging.info(\"Successfully created table: \" + table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Query for all table names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-06 19:30:01,495 [INFO] Found the following table names:\n",
      "2017-03-06 19:30:01,497 [INFO] test_table_1\n",
      "2017-03-06 19:30:01,499 [INFO] test_table_1.json\n",
      "2017-03-06 19:30:01,501 [INFO] test_table_1.orc\n",
      "2017-03-06 19:30:01,502 [INFO] test_table_1.parquet\n",
      "2017-03-06 19:30:01,519 [INFO] test_table_2\n",
      "2017-03-06 19:30:01,520 [INFO] test_table_2.json\n",
      "2017-03-06 19:30:01,520 [INFO] test_table_2.orc\n",
      "2017-03-06 19:30:01,521 [INFO] test_table_2.parquet\n"
     ]
    }
   ],
   "source": [
    "table_name_list = spark.catalog.listTables(database_name)  # return a list of names of tables in the database\n",
    "tables_dataframe = spark.sql(\"SHOW TABLES\").toPandas()  # alternatively return a DataFrame containing names of tables in the given database\n",
    "cols = [c for c in map(str.lower, map(underscore, tables_dataframe.columns))]  # camelcase to underscore to lowercase\n",
    "tables_dataframe.columns = cols\n",
    "logging.info('Found the following table names:')\n",
    "tables_dataframe.rename(columns={'database': 'database_name'}, inplace=True)\n",
    "for table_name in tables_dataframe['table_name']:\n",
    "    logging.info(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Query for the number of records in each table and categorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-06 19:30:24,254 [INFO]              table_name  table_row_count table_size_category\n",
      "0          test_table_1           460949              Medium\n",
      "1     test_table_1.json           460949              Medium\n",
      "2      test_table_1.orc           460949              Medium\n",
      "3  test_table_1.parquet           460949              Medium\n",
      "4          test_table_2              120               Small\n",
      "5     test_table_2.json              120               Small\n",
      "6      test_table_2.orc              120               Small\n",
      "7  test_table_2.parquet              120               Small\n"
     ]
    }
   ],
   "source": [
    "tables_dataframe['table_row_count'] = tables_dataframe['table_name'].apply(\n",
    "    lambda table_name: spark.table(\"`\" + table_name + \"`\").count())\n",
    "bins = [0, 100000, 1000000, 10000000, 1000000000]\n",
    "label_names = ['Small', 'Medium', 'Large', 'X-Large']\n",
    "tables_dataframe['table_size_category'] = pandas.cut(tables_dataframe['table_row_count'], bins,\n",
    "                                                     labels=label_names)\n",
    "logging.info(tables_dataframe[['table_name', 'table_row_count', 'table_size_category']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Benchmarking Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def benchmark():\n",
    "    benchmark_dataframe = pandas.DataFrame()\n",
    "    for table_index, table_row in tables_dataframe.iterrows():\n",
    "        logging.info(\"Benchmarking table: \" + table_row['table_name'])\n",
    "        for query_index, query_row in queries_dataframe.iterrows():\n",
    "            \n",
    "            # Query builder\n",
    "            datatypes_dataframe = pandas.DataFrame.from_dict(spark.table('`' + str(table_row['table_name']) + '`').schema.jsonValue()['fields'])\n",
    "            numeric_columns = datatypes_dataframe[datatypes_dataframe['type'].str.lower().isin(['integer', 'long', 'double'])]['name']\n",
    "            character_columns = datatypes_dataframe[datatypes_dataframe['type'].str.lower().isin(['string'])]['name']  # non-numeric columns\n",
    "            if (numeric_columns.empty or character_columns.empty):\n",
    "                raise AssertionError(table_row['table_name'] + \" needs to have both numeric and character columns.\")\n",
    "            query_builder_dict = {}\n",
    "            query_builder_dict['columns'] = ', '.join(map(str, list(character_columns.sample(n=randint(1, character_columns.size)))))\n",
    "            query_builder_dict['table'] = '`' + table_row['table_name'] + '`'\n",
    "            query_builder_dict['column_1'] = character_columns.sample().to_string(header=False, index=False)\n",
    "            query_builder_dict['column_2'] = character_columns.sample().to_string(header=False, index=False)\n",
    "            query_builder_dict['row'] = str(randint(1, rows))\n",
    "            query_builder_dict['order_column'] = character_columns.sample().to_string(header=False, index=False)\n",
    "            query_builder_dict['numeric_column'] = numeric_columns.sample().to_string(header=False, index=False)\n",
    "            query_builder_dict['column'] = character_columns.sample().to_string(header=False, index=False)\n",
    "\n",
    "            # Benchmark\n",
    "            sql = query_row['query_template'].format(**query_builder_dict)\n",
    "            with Timer() as t:\n",
    "                logging.debug(sql)\n",
    "                dataframe = spark.sql(sql)\n",
    "                query_row['rows'] = dataframe.count()\n",
    "            logging.info(\"Query \" + str(query_row['query_id']) + str(':  {:f} sec'.format(t.interval)))\n",
    "            query_row['time'] = float(t.interval)\n",
    "            query_row['query_executed'] = sql\n",
    "            query_row = pandas.concat([query_row, table_row])\n",
    "            benchmark_dataframe = benchmark_dataframe.append(query_row, ignore_index=True)\n",
    "    benchmark_dataframe.to_csv(csv_filepath, index=False, mode='a', header=not os.path.isfile(csv_filepath))\n",
    "    del benchmark_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-06 19:32:13,266 [INFO] Benchmarking table: test_table_1\n",
      "2017-03-06 19:32:17,091 [INFO] Query 1:  3.784965 sec\n",
      "2017-03-06 19:32:20,759 [INFO] Query 2:  3.597519 sec\n",
      "2017-03-06 19:32:24,670 [INFO] Query 3:  3.845127 sec\n",
      "2017-03-06 19:32:28,583 [INFO] Query 4:  3.866383 sec\n",
      "2017-03-06 19:32:32,031 [INFO] Query 5:  3.382424 sec\n",
      "2017-03-06 19:32:35,576 [INFO] Query 6:  3.495031 sec\n",
      "2017-03-06 19:32:39,139 [INFO] Query 7:  3.506386 sec\n",
      "2017-03-06 19:32:42,543 [INFO] Query 8:  3.279761 sec\n",
      "2017-03-06 19:32:45,909 [INFO] Query 9:  3.321665 sec\n",
      "2017-03-06 19:32:52,261 [INFO] Query 10:  6.301785 sec\n",
      "2017-03-06 19:32:55,999 [INFO] Query 11:  3.632919 sec\n",
      "2017-03-06 19:32:56,264 [INFO] Query 12:  0.215368 sec\n",
      "2017-03-06 19:33:00,095 [INFO] Query 13:  3.757411 sec\n",
      "2017-03-06 19:33:03,536 [INFO] Query 14:  3.395210 sec\n",
      "2017-03-06 19:33:03,914 [INFO] Query 15:  0.323767 sec\n",
      "2017-03-06 19:33:04,175 [INFO] Query 16:  0.210809 sec\n",
      "2017-03-06 19:33:07,794 [INFO] Query 17:  3.553201 sec\n",
      "2017-03-06 19:33:11,362 [INFO] Query 18:  3.528273 sec\n",
      "2017-03-06 19:33:14,926 [INFO] Query 19:  3.471829 sec\n",
      "2017-03-06 19:33:18,721 [INFO] Query 20:  3.719609 sec\n",
      "2017-03-06 19:33:31,007 [INFO] Query 21:  12.216270 sec\n",
      "2017-03-06 19:33:44,102 [INFO] Query 22:  13.046521 sec\n",
      "2017-03-06 19:33:44,458 [INFO] Query 23:  0.282421 sec\n",
      "2017-03-06 19:33:48,940 [INFO] Query 24:  4.410465 sec\n",
      "2017-03-06 19:33:52,931 [INFO] Query 25:  3.936008 sec\n",
      "2017-03-06 19:33:56,416 [INFO] Query 26:  3.407512 sec\n",
      "2017-03-06 19:34:03,596 [INFO] Query 27:  7.115292 sec\n",
      "2017-03-06 19:34:11,187 [INFO] Query 28:  7.531629 sec\n",
      "2017-03-06 19:34:18,008 [INFO] Query 29:  6.785415 sec\n",
      "2017-03-06 19:45:34,469 [INFO] Query 30:  676.367496 sec\n",
      "2017-03-06 19:45:34,480 [INFO] Benchmarking table: test_table_1.json\n",
      "2017-03-06 19:45:40,942 [INFO] Query 1:  6.437083 sec\n",
      "2017-03-06 19:45:47,118 [INFO] Query 2:  6.137408 sec\n",
      "2017-03-06 19:45:53,932 [INFO] Query 3:  6.742814 sec\n",
      "2017-03-06 19:46:00,883 [INFO] Query 4:  6.889241 sec\n",
      "2017-03-06 19:46:07,897 [INFO] Query 5:  6.964419 sec\n",
      "2017-03-06 19:46:14,269 [INFO] Query 6:  6.334419 sec\n",
      "2017-03-06 19:46:20,726 [INFO] Query 7:  6.398903 sec\n",
      "2017-03-06 19:46:26,975 [INFO] Query 8:  6.206435 sec\n",
      "2017-03-06 19:46:34,003 [INFO] Query 9:  6.970915 sec\n",
      "2017-03-06 19:46:43,100 [INFO] Query 10:  9.043335 sec\n",
      "2017-03-06 19:46:49,429 [INFO] Query 11:  6.242626 sec\n",
      "2017-03-06 19:46:49,866 [INFO] Query 12:  0.379502 sec\n",
      "2017-03-06 19:46:56,888 [INFO] Query 13:  6.954290 sec\n",
      "2017-03-06 19:47:03,754 [INFO] Query 14:  6.810468 sec\n",
      "2017-03-06 19:47:03,960 [INFO] Query 15:  0.152174 sec\n",
      "2017-03-06 19:47:04,150 [INFO] Query 16:  0.145780 sec\n",
      "2017-03-06 19:47:11,243 [INFO] Query 17:  7.038342 sec\n",
      "2017-03-06 19:47:17,743 [INFO] Query 18:  6.466745 sec\n",
      "2017-03-06 19:47:24,722 [INFO] Query 19:  6.890854 sec\n",
      "2017-03-06 19:47:31,535 [INFO] Query 20:  6.756323 sec\n",
      "2017-03-06 19:47:55,610 [INFO] Query 21:  24.041726 sec\n",
      "2017-03-06 19:48:14,834 [INFO] Query 22:  19.187738 sec\n",
      "2017-03-06 19:48:15,188 [INFO] Query 23:  0.320984 sec\n",
      "2017-03-06 19:48:24,933 [INFO] Query 24:  9.648049 sec\n",
      "2017-03-06 19:48:37,534 [INFO] Query 25:  12.476897 sec\n",
      "2017-03-06 19:48:46,566 [INFO] Query 26:  8.978777 sec\n",
      "2017-03-06 19:52:31,163 [INFO] Query 27:  224.525143 sec\n",
      "2017-03-06 19:52:40,267 [INFO] Query 28:  9.081510 sec\n",
      "2017-03-06 19:52:56,764 [INFO] Query 29:  16.461522 sec\n",
      "2017-03-06 19:53:15,745 [INFO] Query 30:  18.953250 sec\n",
      "2017-03-06 19:53:15,754 [INFO] Benchmarking table: test_table_1.orc\n",
      "2017-03-06 19:53:16,075 [INFO] Query 1:  0.274048 sec\n",
      "2017-03-06 19:53:16,306 [INFO] Query 2:  0.189931 sec\n",
      "2017-03-06 19:53:16,561 [INFO] Query 3:  0.227081 sec\n",
      "2017-03-06 19:53:16,932 [INFO] Query 4:  0.320501 sec\n",
      "2017-03-06 19:53:17,197 [INFO] Query 5:  0.181794 sec\n",
      "2017-03-06 19:53:17,405 [INFO] Query 6:  0.178134 sec\n",
      "2017-03-06 19:53:17,712 [INFO] Query 7:  0.279538 sec\n",
      "2017-03-06 19:53:18,029 [INFO] Query 8:  0.249303 sec\n",
      "2017-03-06 19:53:18,277 [INFO] Query 9:  0.172426 sec\n",
      "2017-03-06 19:53:20,063 [INFO] Query 10:  1.739912 sec\n",
      "2017-03-06 19:53:20,331 [INFO] Query 11:  0.237119 sec\n",
      "2017-03-06 19:53:20,472 [INFO] Query 12:  0.115604 sec\n",
      "2017-03-06 19:53:20,979 [INFO] Query 13:  0.461573 sec\n",
      "2017-03-06 19:53:21,523 [INFO] Query 14:  0.503647 sec\n",
      "2017-03-06 19:53:21,713 [INFO] Query 15:  0.165155 sec\n",
      "2017-03-06 19:53:21,870 [INFO] Query 16:  0.114689 sec\n",
      "2017-03-06 19:53:22,283 [INFO] Query 17:  0.366174 sec\n",
      "2017-03-06 19:53:22,680 [INFO] Query 18:  0.350181 sec\n",
      "2017-03-06 19:53:23,064 [INFO] Query 19:  0.356844 sec\n",
      "2017-03-06 19:53:23,445 [INFO] Query 20:  0.354412 sec\n",
      "2017-03-06 19:53:29,837 [INFO] Query 21:  6.357069 sec\n",
      "2017-03-06 19:53:34,690 [INFO] Query 22:  4.827069 sec\n",
      "2017-03-06 19:53:35,079 [INFO] Query 23:  0.362813 sec\n",
      "2017-03-06 19:53:37,358 [INFO] Query 24:  2.240673 sec\n",
      "2017-03-06 19:53:37,817 [INFO] Query 25:  0.395598 sec\n",
      "2017-03-06 19:53:38,383 [INFO] Query 26:  0.540826 sec\n",
      "2017-03-06 19:53:39,217 [INFO] Query 27:  0.722659 sec\n",
      "2017-03-06 19:53:39,981 [INFO] Query 28:  0.727725 sec\n",
      "2017-03-06 19:53:41,464 [INFO] Query 29:  1.445685 sec\n",
      "2017-03-06 19:53:42,221 [INFO] Query 30:  0.718458 sec\n",
      "2017-03-06 19:53:42,231 [INFO] Benchmarking table: test_table_1.parquet\n",
      "2017-03-06 19:53:43,024 [INFO] Query 1:  0.759763 sec\n",
      "2017-03-06 19:53:43,146 [INFO] Query 2:  0.092717 sec\n",
      "2017-03-06 19:53:43,287 [INFO] Query 3:  0.101117 sec\n",
      "2017-03-06 19:53:43,416 [INFO] Query 4:  0.086557 sec\n",
      "2017-03-06 19:53:43,586 [INFO] Query 5:  0.142955 sec\n",
      "2017-03-06 19:53:43,727 [INFO] Query 6:  0.112277 sec\n",
      "2017-03-06 19:53:44,019 [INFO] Query 7:  0.265187 sec\n",
      "2017-03-06 19:53:44,233 [INFO] Query 8:  0.166177 sec\n",
      "2017-03-06 19:53:44,357 [INFO] Query 9:  0.097349 sec\n",
      "2017-03-06 19:53:46,011 [INFO] Query 10:  1.621421 sec\n",
      "2017-03-06 19:53:46,155 [INFO] Query 11:  0.116976 sec\n",
      "2017-03-06 19:53:46,267 [INFO] Query 12:  0.085843 sec\n",
      "2017-03-06 19:53:46,535 [INFO] Query 13:  0.235400 sec\n",
      "2017-03-06 19:53:46,873 [INFO] Query 14:  0.300810 sec\n",
      "2017-03-06 19:53:47,035 [INFO] Query 15:  0.126870 sec\n",
      "2017-03-06 19:53:47,295 [INFO] Query 16:  0.230636 sec\n",
      "2017-03-06 19:53:47,560 [INFO] Query 17:  0.213985 sec\n",
      "2017-03-06 19:53:47,674 [INFO] Query 18:  0.077559 sec\n",
      "2017-03-06 19:53:48,464 [INFO] Query 19:  0.734535 sec\n",
      "2017-03-06 19:53:48,633 [INFO] Query 20:  0.142128 sec\n",
      "2017-03-06 19:53:59,660 [INFO] Query 21:  10.970894 sec\n",
      "2017-03-06 19:54:04,476 [INFO] Query 22:  4.561763 sec\n",
      "2017-03-06 19:54:04,711 [INFO] Query 23:  0.173650 sec\n",
      "2017-03-06 19:54:05,044 [INFO] Query 24:  0.294089 sec\n",
      "2017-03-06 19:54:05,474 [INFO] Query 25:  0.404083 sec\n",
      "2017-03-06 19:54:05,881 [INFO] Query 26:  0.343744 sec\n",
      "2017-03-06 19:54:06,543 [INFO] Query 27:  0.610989 sec\n",
      "2017-03-06 19:54:06,852 [INFO] Query 28:  0.276061 sec\n",
      "2017-03-06 19:54:07,207 [INFO] Query 29:  0.323810 sec\n",
      "2017-03-06 19:54:07,992 [INFO] Query 30:  0.748972 sec\n",
      "2017-03-06 19:54:08,001 [INFO] Benchmarking table: test_table_2\n",
      "2017-03-06 19:54:08,110 [INFO] Query 1:  0.080699 sec\n",
      "2017-03-06 19:54:08,198 [INFO] Query 2:  0.070569 sec\n",
      "2017-03-06 19:54:08,285 [INFO] Query 3:  0.067948 sec\n",
      "2017-03-06 19:54:08,408 [INFO] Query 4:  0.105694 sec\n",
      "2017-03-06 19:54:08,505 [INFO] Query 5:  0.043501 sec\n",
      "2017-03-06 19:54:08,571 [INFO] Query 6:  0.045177 sec\n",
      "2017-03-06 19:54:08,682 [INFO] Query 7:  0.073113 sec\n",
      "2017-03-06 19:54:08,751 [INFO] Query 8:  0.048643 sec\n",
      "2017-03-06 19:54:08,823 [INFO] Query 9:  0.050379 sec\n",
      "2017-03-06 19:54:09,749 [INFO] Query 10:  0.908613 sec\n",
      "2017-03-06 19:54:09,824 [INFO] Query 11:  0.059032 sec\n",
      "2017-03-06 19:54:09,969 [INFO] Query 12:  0.126615 sec\n",
      "2017-03-06 19:54:10,048 [INFO] Query 13:  0.061131 sec\n",
      "2017-03-06 19:54:10,147 [INFO] Query 14:  0.081832 sec\n",
      "2017-03-06 19:54:10,229 [INFO] Query 15:  0.052872 sec\n",
      "2017-03-06 19:54:10,339 [INFO] Query 16:  0.088027 sec\n",
      "2017-03-06 19:54:10,459 [INFO] Query 17:  0.101133 sec\n",
      "2017-03-06 19:54:10,553 [INFO] Query 18:  0.075164 sec\n",
      "2017-03-06 19:54:10,635 [INFO] Query 19:  0.064190 sec\n",
      "2017-03-06 19:54:10,719 [INFO] Query 20:  0.065309 sec\n",
      "2017-03-06 19:54:11,841 [INFO] Query 21:  1.104960 sec\n",
      "2017-03-06 19:54:12,688 [INFO] Query 22:  0.829194 sec\n",
      "2017-03-06 19:54:12,783 [INFO] Query 23:  0.072108 sec\n",
      "2017-03-06 19:54:13,082 [INFO] Query 24:  0.275178 sec\n",
      "2017-03-06 19:54:13,247 [INFO] Query 25:  0.127781 sec\n",
      "2017-03-06 19:54:13,423 [INFO] Query 26:  0.158159 sec\n",
      "2017-03-06 19:54:13,601 [INFO] Query 27:  0.162050 sec\n",
      "2017-03-06 19:54:13,802 [INFO] Query 28:  0.183362 sec\n",
      "2017-03-06 19:54:13,932 [INFO] Query 29:  0.105383 sec\n",
      "2017-03-06 19:54:14,121 [INFO] Query 30:  0.158709 sec\n",
      "2017-03-06 19:54:14,131 [INFO] Benchmarking table: test_table_2.json\n",
      "2017-03-06 19:54:14,241 [INFO] Query 1:  0.058648 sec\n",
      "2017-03-06 19:54:14,325 [INFO] Query 2:  0.060638 sec\n",
      "2017-03-06 19:54:14,418 [INFO] Query 3:  0.075045 sec\n",
      "2017-03-06 19:54:14,522 [INFO] Query 4:  0.065606 sec\n",
      "2017-03-06 19:54:14,595 [INFO] Query 5:  0.055773 sec\n",
      "2017-03-06 19:54:14,695 [INFO] Query 6:  0.081539 sec\n",
      "2017-03-06 19:54:14,778 [INFO] Query 7:  0.055505 sec\n",
      "2017-03-06 19:54:14,869 [INFO] Query 8:  0.054801 sec\n",
      "2017-03-06 19:54:14,958 [INFO] Query 9:  0.061652 sec\n",
      "2017-03-06 19:54:15,786 [INFO] Query 10:  0.799619 sec\n",
      "2017-03-06 19:54:15,863 [INFO] Query 11:  0.054854 sec\n",
      "2017-03-06 19:54:15,941 [INFO] Query 12:  0.048063 sec\n",
      "2017-03-06 19:54:16,044 [INFO] Query 13:  0.057170 sec\n",
      "2017-03-06 19:54:16,172 [INFO] Query 14:  0.094093 sec\n",
      "2017-03-06 19:54:16,242 [INFO] Query 15:  0.048189 sec\n",
      "2017-03-06 19:54:16,401 [INFO] Query 16:  0.143046 sec\n",
      "2017-03-06 19:54:16,572 [INFO] Query 17:  0.153151 sec\n",
      "2017-03-06 19:54:16,687 [INFO] Query 18:  0.095908 sec\n",
      "2017-03-06 19:54:16,778 [INFO] Query 19:  0.070153 sec\n",
      "2017-03-06 19:54:17,141 [INFO] Query 20:  0.345968 sec\n",
      "2017-03-06 19:54:18,046 [INFO] Query 21:  0.887299 sec\n",
      "2017-03-06 19:54:18,949 [INFO] Query 22:  0.886029 sec\n",
      "2017-03-06 19:54:19,077 [INFO] Query 23:  0.090884 sec\n",
      "2017-03-06 19:54:19,251 [INFO] Query 24:  0.156371 sec\n",
      "2017-03-06 19:54:19,444 [INFO] Query 25:  0.161723 sec\n",
      "2017-03-06 19:54:19,594 [INFO] Query 26:  0.128475 sec\n",
      "2017-03-06 19:54:19,846 [INFO] Query 27:  0.232422 sec\n",
      "2017-03-06 19:54:20,044 [INFO] Query 28:  0.180470 sec\n",
      "2017-03-06 19:54:20,144 [INFO] Query 29:  0.081406 sec\n",
      "2017-03-06 19:54:20,267 [INFO] Query 30:  0.106769 sec\n",
      "2017-03-06 19:54:20,277 [INFO] Benchmarking table: test_table_2.orc\n",
      "2017-03-06 19:54:20,424 [INFO] Query 1:  0.133983 sec\n",
      "2017-03-06 19:54:20,507 [INFO] Query 2:  0.066364 sec\n",
      "2017-03-06 19:54:20,643 [INFO] Query 3:  0.103666 sec\n",
      "2017-03-06 19:54:20,715 [INFO] Query 4:  0.054726 sec\n",
      "2017-03-06 19:54:20,808 [INFO] Query 5:  0.073193 sec\n",
      "2017-03-06 19:54:20,893 [INFO] Query 6:  0.062001 sec\n",
      "2017-03-06 19:54:20,969 [INFO] Query 7:  0.052293 sec\n",
      "2017-03-06 19:54:21,049 [INFO] Query 8:  0.061295 sec\n",
      "2017-03-06 19:54:21,134 [INFO] Query 9:  0.065138 sec\n",
      "2017-03-06 19:54:22,181 [INFO] Query 10:  1.010457 sec\n",
      "2017-03-06 19:54:22,413 [INFO] Query 11:  0.214662 sec\n",
      "2017-03-06 19:54:22,502 [INFO] Query 12:  0.061289 sec\n",
      "2017-03-06 19:54:22,587 [INFO] Query 13:  0.065493 sec\n",
      "2017-03-06 19:54:22,677 [INFO] Query 14:  0.068821 sec\n",
      "2017-03-06 19:54:22,871 [INFO] Query 15:  0.165214 sec\n",
      "2017-03-06 19:54:22,956 [INFO] Query 16:  0.067874 sec\n",
      "2017-03-06 19:54:23,063 [INFO] Query 17:  0.072827 sec\n",
      "2017-03-06 19:54:23,222 [INFO] Query 18:  0.095826 sec\n",
      "2017-03-06 19:54:23,395 [INFO] Query 19:  0.116051 sec\n",
      "2017-03-06 19:54:23,491 [INFO] Query 20:  0.079468 sec\n",
      "2017-03-06 19:54:25,085 [INFO] Query 21:  1.548342 sec\n",
      "2017-03-06 19:54:26,028 [INFO] Query 22:  0.911567 sec\n",
      "2017-03-06 19:54:26,243 [INFO] Query 23:  0.193803 sec\n",
      "2017-03-06 19:54:26,433 [INFO] Query 24:  0.160604 sec\n",
      "2017-03-06 19:54:26,686 [INFO] Query 25:  0.223147 sec\n",
      "2017-03-06 19:54:26,893 [INFO] Query 26:  0.190798 sec\n",
      "2017-03-06 19:54:27,140 [INFO] Query 27:  0.220742 sec\n",
      "2017-03-06 19:54:27,334 [INFO] Query 28:  0.175658 sec\n",
      "2017-03-06 19:54:27,575 [INFO] Query 29:  0.204300 sec\n",
      "2017-03-06 19:54:27,766 [INFO] Query 30:  0.135481 sec\n",
      "2017-03-06 19:54:27,777 [INFO] Benchmarking table: test_table_2.parquet\n",
      "2017-03-06 19:54:27,973 [INFO] Query 1:  0.183151 sec\n",
      "2017-03-06 19:54:28,065 [INFO] Query 2:  0.062938 sec\n",
      "2017-03-06 19:54:28,180 [INFO] Query 3:  0.089093 sec\n",
      "2017-03-06 19:54:28,354 [INFO] Query 4:  0.137348 sec\n",
      "2017-03-06 19:54:28,467 [INFO] Query 5:  0.083863 sec\n",
      "2017-03-06 19:54:28,542 [INFO] Query 6:  0.055587 sec\n",
      "2017-03-06 19:54:28,651 [INFO] Query 7:  0.073201 sec\n",
      "2017-03-06 19:54:28,795 [INFO] Query 8:  0.123416 sec\n",
      "2017-03-06 19:54:28,888 [INFO] Query 9:  0.073470 sec\n",
      "2017-03-06 19:54:29,735 [INFO] Query 10:  0.754702 sec\n",
      "2017-03-06 19:54:29,811 [INFO] Query 11:  0.054701 sec\n",
      "2017-03-06 19:54:29,904 [INFO] Query 12:  0.059858 sec\n",
      "2017-03-06 19:54:30,017 [INFO] Query 13:  0.095132 sec\n",
      "2017-03-06 19:54:30,155 [INFO] Query 14:  0.122431 sec\n",
      "2017-03-06 19:54:30,282 [INFO] Query 15:  0.092727 sec\n",
      "2017-03-06 19:54:30,361 [INFO] Query 16:  0.060377 sec\n",
      "2017-03-06 19:54:30,480 [INFO] Query 17:  0.059261 sec\n",
      "2017-03-06 19:54:30,566 [INFO] Query 18:  0.067166 sec\n",
      "2017-03-06 19:54:30,673 [INFO] Query 19:  0.081343 sec\n",
      "2017-03-06 19:54:30,756 [INFO] Query 20:  0.064324 sec\n",
      "2017-03-06 19:54:31,621 [INFO] Query 21:  0.824499 sec\n",
      "2017-03-06 19:54:32,439 [INFO] Query 22:  0.799587 sec\n",
      "2017-03-06 19:54:32,753 [INFO] Query 23:  0.276964 sec\n",
      "2017-03-06 19:54:32,927 [INFO] Query 24:  0.138156 sec\n",
      "2017-03-06 19:54:33,056 [INFO] Query 25:  0.109829 sec\n",
      "2017-03-06 19:54:33,295 [INFO] Query 26:  0.209222 sec\n",
      "2017-03-06 19:54:33,514 [INFO] Query 27:  0.195911 sec\n",
      "2017-03-06 19:54:33,715 [INFO] Query 28:  0.181017 sec\n",
      "2017-03-06 19:54:33,915 [INFO] Query 29:  0.167185 sec\n",
      "2017-03-06 19:54:34,093 [INFO] Query 30:  0.161163 sec\n",
      "2017-03-06 19:54:34,126 [INFO] Spark SQL benchmark time: 1340.8608796 sec\n"
     ]
    }
   ],
   "source": [
    "csv_filepath = 'tempresults.csv'\n",
    "rows = 10  # maximum number of rows to return from each query execution.\n",
    "database = 'Spark SQL'\n",
    "queries_filepath = 'queries/queries.csv'\n",
    "queries_dataframe = pandas.read_csv(queries_filepath)  # load queries from CSV file\n",
    "queries_dataframe = queries_dataframe[\n",
    "    queries_dataframe['database'] == database]  # filter queries on database name\n",
    "if not queries_dataframe.empty:\n",
    "    with Timer() as t:\n",
    "        benchmark()\n",
    "    logging.info(database + ' benchmark time: %.07f sec' % t.interval)\n",
    "else:\n",
    "    logging.warning(\"Missing \" + database + \" queries from \" + queries_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
