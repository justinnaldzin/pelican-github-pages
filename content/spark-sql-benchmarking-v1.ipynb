{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Spark SQL Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Apache Spark v1.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define SparkContext and SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "from random import randint\n",
    "import json\n",
    "import logging\n",
    "import pandas\n",
    "from inflection import underscore\n",
    "from datetime import datetime, timezone\n",
    "from threading import Thread\n",
    "from sqlalchemy import types\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "\n",
    "conf = SparkConf().setAppName('Spark SQL Benchmarking').set(\"spark.executor.memory\", \"2g\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create database\n",
    "database_name = 'spark_sql_benchmarking'\n",
    "sqlContext.sql('CREATE DATABASE IF NOT EXISTS ' + database_name)\n",
    "sqlContext.sql('USE ' + database_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Configure logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "\n",
    "handler = logging.StreamHandler()\n",
    "handler.setLevel(logging.INFO)\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Timer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.end = time.perf_counter()\n",
    "        self.interval = self.end - self.start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Search for data files on local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-06 18:21:10,759 [INFO] Found the following files in path:  /jupyter-vagrant/notebook/data\n",
      "2017-03-06 18:21:10,761 [INFO] /jupyter-vagrant/notebook/data/test_table_1.csv\n",
      "2017-03-06 18:21:10,763 [INFO] /jupyter-vagrant/notebook/data/test_table_2.csv\n"
     ]
    }
   ],
   "source": [
    "# full directory path to where the data files are stored.  These will be used to create the tables and insert into database.\n",
    "data_path = os.path.join(os.path.abspath(os.curdir), 'data')\n",
    "\n",
    "data_filepath_list = [os.path.join(data_path, filename) for filename in os.listdir(data_path) if filename.endswith(\".csv\")]\n",
    "if not data_filepath_list:\n",
    "    logging.error(\"No data files found in path:  \" + str(data_path))\n",
    "    sys.exit(1)\n",
    "tables_dataframe = pandas.DataFrame({'table_name': [(os.path.splitext(os.path.basename(filename))[0])\n",
    "                                                    for filename in data_filepath_list]})\n",
    "logging.info('Found the following files in path:  ' + str(data_path))\n",
    "for filename in data_filepath_list:\n",
    "    logging.info(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load data into Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-06 18:21:10,814 [INFO] Reading data file: /jupyter-vagrant/notebook/data/test_table_1.csv\n",
      "2017-03-06 18:21:28,161 [INFO] 460949 rows loaded.\n",
      "2017-03-06 18:21:28,377 [INFO] Registering the csv source type as a SQL temporary view...\n",
      "2017-03-06 18:21:28,414 [INFO] Successfully created table: test_table_1\n",
      "2017-03-06 18:21:28,416 [INFO] Saving the DataFrame into alternative data source types:\n",
      "2017-03-06 18:21:28,416 [INFO] Saving test_table_1.parquet...\n",
      "2017-03-06 18:21:48,590 [INFO] Registering the parquet source type as a SQL temporary view...\n",
      "2017-03-06 18:21:48,989 [INFO] Successfully created table: test_table_1.parquet\n",
      "2017-03-06 18:21:48,991 [INFO] Saving test_table_1.json...\n",
      "2017-03-06 18:22:06,200 [INFO] Registering the json source type as a SQL temporary view...\n",
      "2017-03-06 18:22:19,179 [INFO] Successfully created table: test_table_1.json\n",
      "2017-03-06 18:22:19,181 [INFO] Saving test_table_1.orc...\n",
      "2017-03-06 18:22:45,112 [INFO] Registering the orc source type as a SQL temporary view...\n",
      "2017-03-06 18:22:45,313 [INFO] Successfully created table: test_table_1.orc\n",
      "2017-03-06 18:22:45,316 [INFO] Reading data file: /jupyter-vagrant/notebook/data/test_table_2.csv\n",
      "2017-03-06 18:22:45,926 [INFO] 120 rows loaded.\n",
      "2017-03-06 18:22:45,935 [INFO] Registering the csv source type as a SQL temporary view...\n",
      "2017-03-06 18:22:45,943 [INFO] Successfully created table: test_table_2\n",
      "2017-03-06 18:22:45,945 [INFO] Saving the DataFrame into alternative data source types:\n",
      "2017-03-06 18:22:45,946 [INFO] Saving test_table_2.parquet...\n",
      "2017-03-06 18:22:47,376 [INFO] Registering the parquet source type as a SQL temporary view...\n",
      "2017-03-06 18:22:47,659 [INFO] Successfully created table: test_table_2.parquet\n",
      "2017-03-06 18:22:47,661 [INFO] Saving test_table_2.json...\n",
      "2017-03-06 18:22:48,065 [INFO] Registering the json source type as a SQL temporary view...\n",
      "2017-03-06 18:22:48,166 [INFO] Successfully created table: test_table_2.json\n",
      "2017-03-06 18:22:48,168 [INFO] Saving test_table_2.orc...\n",
      "2017-03-06 18:22:48,554 [INFO] Registering the orc source type as a SQL temporary view...\n",
      "2017-03-06 18:22:49,006 [INFO] Successfully created table: test_table_2.orc\n"
     ]
    }
   ],
   "source": [
    "for filepath in data_filepath_list:\n",
    "    logging.info(\"Reading data file: \" + filepath)\n",
    "    table_name = os.path.splitext(os.path.basename(filepath))[0] #  set table name to basename of filepath\n",
    "    \n",
    "    # Create an external table from CSV\n",
    "    #sqlContext.createExternalTable(table_name, path=filepath, source='csv', header='true', inferschema='true')\n",
    "    \n",
    "    # The following is needed to rename columns or choose specific columns:\n",
    "    if True:\n",
    "        # Read CSV into Spark DataFrame\n",
    "        dataframe = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(filepath)\n",
    "        logging.info(str(dataframe.count()) + \" rows loaded.\")\n",
    "        #dataframe.dropna(axis=1, thresh=dataframe.count()//10, inplace=True)  # drop columns having over 90% NaN values\n",
    "        cols = [c for c in map(str.upper, map(underscore, dataframe.columns))]  # camelcase to underscore to uppercase\n",
    "        dataframe = dataframe.toDF(*cols)\n",
    "        #dataframe.cache()  # cache data in memory\n",
    "\n",
    "        # Create temporary view from CSV\n",
    "        logging.info(\"Registering the csv source type as a SQL temporary view...\")\n",
    "        dataframe.createOrReplaceTempView(table_name)\n",
    "        #sqlContext.registerDataFrameAsTable(dataframe, table_name)  # alternative syntax; throws exception if table already exists\n",
    "        logging.info(\"Successfully created table: \" + table_name)\n",
    "    \n",
    "    # Save as alternative data source types and create temporary views\n",
    "    logging.info(\"Saving the DataFrame into alternative data source types:\")\n",
    "    data_source_types = ['parquet', 'json', 'orc']\n",
    "    for source_type in data_source_types:\n",
    "        table_path = table_name + \".\" + source_type\n",
    "        logging.info(\"Saving \" + table_path + \"...\")\n",
    "        dataframe.write.save(table_path, format=source_type, mode='overwrite')\n",
    "        logging.info(\"Registering the \" + source_type + \" source type as a SQL temporary view...\")            \n",
    "        temp_dataframe = getattr(sqlContext.read, source_type)(table_path)\n",
    "        temp_dataframe.createOrReplaceTempView('`' + table_path + '`')\n",
    "        logging.info(\"Successfully created table: \" + table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Query for all table names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-06 18:22:50,096 [INFO] Found the following table names:\n",
      "2017-03-06 18:22:50,099 [INFO] test_table_1\n",
      "2017-03-06 18:22:50,101 [INFO] test_table_1.json\n",
      "2017-03-06 18:22:50,103 [INFO] test_table_1.orc\n",
      "2017-03-06 18:22:50,104 [INFO] test_table_1.parquet\n",
      "2017-03-06 18:22:50,106 [INFO] test_table_2\n",
      "2017-03-06 18:22:50,106 [INFO] test_table_2.json\n",
      "2017-03-06 18:22:50,107 [INFO] test_table_2.orc\n",
      "2017-03-06 18:22:50,110 [INFO] test_table_2.parquet\n"
     ]
    }
   ],
   "source": [
    "table_name_list = sqlContext.tableNames(database_name)  # return a list of names of tables in the database\n",
    "tables_dataframe = sqlContext.tables(database_name).toPandas()  # alternatively return a DataFrame containing names of tables in the given database\n",
    "cols = [c for c in map(str.lower, map(underscore, tables_dataframe.columns))]  # camelcase to underscore to lowercase\n",
    "tables_dataframe.columns = cols\n",
    "logging.info('Found the following table names:')\n",
    "tables_dataframe.rename(columns={'database': 'database_name'}, inplace=True)\n",
    "for table_name in tables_dataframe['table_name']:\n",
    "    logging.info(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Query for the number of records in each table and categorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-06 18:23:01,349 [INFO]              table_name  table_row_count table_size_category\n",
      "0          test_table_1           460949              Medium\n",
      "1     test_table_1.json           460949              Medium\n",
      "2      test_table_1.orc           460949              Medium\n",
      "3  test_table_1.parquet           460949              Medium\n",
      "4          test_table_2              120               Small\n",
      "5     test_table_2.json              120               Small\n",
      "6      test_table_2.orc              120               Small\n",
      "7  test_table_2.parquet              120               Small\n"
     ]
    }
   ],
   "source": [
    "tables_dataframe['table_row_count'] = tables_dataframe['table_name'].apply(\n",
    "    lambda table_name: sqlContext.table(\"`\" + table_name + \"`\").count())\n",
    "bins = [0, 100000, 1000000, 10000000, 1000000000]\n",
    "label_names = ['Small', 'Medium', 'Large', 'X-Large']\n",
    "tables_dataframe['table_size_category'] = pandas.cut(tables_dataframe['table_row_count'], bins,\n",
    "                                                     labels=label_names)\n",
    "logging.info(tables_dataframe[['table_name', 'table_row_count', 'table_size_category']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Benchmarking Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def benchmark():\n",
    "    benchmark_dataframe = pandas.DataFrame()\n",
    "    for table_index, table_row in tables_dataframe.iterrows():\n",
    "        logging.info(\"Benchmarking table: \" + table_row['table_name'])\n",
    "        for query_index, query_row in queries_dataframe.iterrows():\n",
    "            \n",
    "            # Query builder\n",
    "            datatypes_dataframe = pandas.DataFrame.from_dict(sqlContext.table('`' + str(table_row['table_name']) + '`').schema.jsonValue()['fields'])\n",
    "            numeric_columns = datatypes_dataframe[datatypes_dataframe['type'].str.lower().isin(['integer', 'long', 'double'])]['name']\n",
    "            character_columns = datatypes_dataframe[datatypes_dataframe['type'].str.lower().isin(['string'])]['name']  # non-numeric columns\n",
    "            if (numeric_columns.empty or character_columns.empty):\n",
    "                raise AssertionError(table_row['table_name'] + \" needs to have both numeric and character columns.\")\n",
    "            query_builder_dict = {}\n",
    "            query_builder_dict['columns'] = ', '.join(map(str, list(character_columns.sample(n=randint(1, character_columns.size)))))\n",
    "            query_builder_dict['table'] = '`' + table_row['table_name'] + '`'\n",
    "            query_builder_dict['column_1'] = character_columns.sample().to_string(header=False, index=False)\n",
    "            query_builder_dict['column_2'] = character_columns.sample().to_string(header=False, index=False)\n",
    "            query_builder_dict['row'] = str(randint(1, rows))\n",
    "            query_builder_dict['order_column'] = character_columns.sample().to_string(header=False, index=False)\n",
    "            query_builder_dict['numeric_column'] = numeric_columns.sample().to_string(header=False, index=False)\n",
    "            query_builder_dict['column'] = character_columns.sample().to_string(header=False, index=False)\n",
    "\n",
    "            # Benchmark\n",
    "            sql = query_row['query_template'].format(**query_builder_dict)\n",
    "            with Timer() as t:\n",
    "                logging.debug(sql)\n",
    "                dataframe = sqlContext.sql(sql)\n",
    "                query_row['rows'] = dataframe.count()\n",
    "            logging.info(\"Query \" + str(query_row['query_id']) + str(':  {:f} sec'.format(t.interval)))\n",
    "            query_row['time'] = float(t.interval)\n",
    "            query_row['query_executed'] = sql\n",
    "            query_row = pandas.concat([query_row, table_row])\n",
    "            benchmark_dataframe = benchmark_dataframe.append(query_row, ignore_index=True)\n",
    "    benchmark_dataframe.to_csv(csv_filepath, index=False, mode='a', header=not os.path.isfile(csv_filepath))\n",
    "    del benchmark_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-06 18:24:37,916 [INFO] Benchmarking table: test_table_1\n",
      "2017-03-06 18:24:41,747 [INFO] Query 1:  3.784569 sec\n",
      "2017-03-06 18:24:45,272 [INFO] Query 2:  3.435986 sec\n",
      "2017-03-06 18:24:48,561 [INFO] Query 3:  3.220679 sec\n",
      "2017-03-06 18:24:52,141 [INFO] Query 4:  3.532028 sec\n",
      "2017-03-06 18:24:55,903 [INFO] Query 5:  3.658149 sec\n",
      "2017-03-06 18:24:59,335 [INFO] Query 6:  3.383215 sec\n",
      "2017-03-06 18:25:02,914 [INFO] Query 7:  3.449616 sec\n",
      "2017-03-06 18:25:06,254 [INFO] Query 8:  3.303245 sec\n",
      "2017-03-06 18:25:09,985 [INFO] Query 9:  3.686394 sec\n",
      "2017-03-06 18:25:16,526 [INFO] Query 10:  6.489271 sec\n",
      "2017-03-06 18:25:19,992 [INFO] Query 11:  3.423821 sec\n",
      "2017-03-06 18:25:20,327 [INFO] Query 12:  0.272422 sec\n",
      "2017-03-06 18:25:24,078 [INFO] Query 13:  3.715508 sec\n",
      "2017-03-06 18:25:27,589 [INFO] Query 14:  3.455086 sec\n",
      "2017-03-06 18:25:27,909 [INFO] Query 15:  0.273223 sec\n",
      "2017-03-06 18:25:28,186 [INFO] Query 16:  0.225212 sec\n",
      "2017-03-06 18:25:31,992 [INFO] Query 17:  3.758462 sec\n",
      "2017-03-06 18:25:35,439 [INFO] Query 18:  3.400007 sec\n",
      "2017-03-06 18:25:39,058 [INFO] Query 19:  3.580909 sec\n",
      "2017-03-06 18:25:43,037 [INFO] Query 20:  3.932191 sec\n",
      "2017-03-06 18:25:55,724 [INFO] Query 21:  12.646464 sec\n",
      "2017-03-06 18:26:20,534 [INFO] Query 22:  24.773668 sec\n",
      "2017-03-06 18:26:21,479 [INFO] Query 23:  0.903557 sec\n",
      "2017-03-06 18:26:25,221 [INFO] Query 24:  3.700274 sec\n",
      "2017-03-06 18:26:29,436 [INFO] Query 25:  4.170478 sec\n",
      "2017-03-06 18:26:36,395 [INFO] Query 26:  6.876760 sec\n",
      "2017-03-06 18:26:43,849 [INFO] Query 27:  7.406655 sec\n",
      "2017-03-06 18:26:51,273 [INFO] Query 28:  7.382196 sec\n",
      "2017-03-06 18:26:56,939 [INFO] Query 29:  5.629171 sec\n",
      "2017-03-06 18:27:02,391 [INFO] Query 30:  5.417879 sec\n",
      "2017-03-06 18:27:02,401 [INFO] Benchmarking table: test_table_1.json\n",
      "2017-03-06 18:27:08,925 [INFO] Query 1:  6.489260 sec\n",
      "2017-03-06 18:27:15,716 [INFO] Query 2:  6.750238 sec\n",
      "2017-03-06 18:27:21,577 [INFO] Query 3:  5.818746 sec\n",
      "2017-03-06 18:27:27,678 [INFO] Query 4:  6.066425 sec\n",
      "2017-03-06 18:27:34,166 [INFO] Query 5:  6.454322 sec\n",
      "2017-03-06 18:27:40,462 [INFO] Query 6:  6.187630 sec\n",
      "2017-03-06 18:27:46,673 [INFO] Query 7:  6.175703 sec\n",
      "2017-03-06 18:27:53,342 [INFO] Query 8:  6.638215 sec\n",
      "2017-03-06 18:27:59,609 [INFO] Query 9:  6.231776 sec\n",
      "2017-03-06 18:28:08,810 [INFO] Query 10:  9.167687 sec\n",
      "2017-03-06 18:28:15,906 [INFO] Query 11:  7.059134 sec\n",
      "2017-03-06 18:28:16,786 [INFO] Query 12:  0.820396 sec\n",
      "2017-03-06 18:28:23,118 [INFO] Query 13:  6.226962 sec\n",
      "2017-03-06 18:28:29,819 [INFO] Query 14:  6.667966 sec\n",
      "2017-03-06 18:28:30,179 [INFO] Query 15:  0.321319 sec\n",
      "2017-03-06 18:28:30,551 [INFO] Query 16:  0.313896 sec\n",
      "2017-03-06 18:28:36,743 [INFO] Query 17:  6.153502 sec\n",
      "2017-03-06 18:28:36,951 [INFO] Query 18:  0.173285 sec\n",
      "2017-03-06 18:28:43,925 [INFO] Query 19:  6.950466 sec\n",
      "2017-03-06 18:28:50,714 [INFO] Query 20:  6.760211 sec\n",
      "2017-03-06 18:29:09,389 [INFO] Query 21:  18.637178 sec\n",
      "2017-03-06 18:29:30,947 [INFO] Query 22:  21.525920 sec\n",
      "2017-03-06 18:29:31,307 [INFO] Query 23:  0.321284 sec\n",
      "2017-03-06 18:29:39,961 [INFO] Query 24:  8.624466 sec\n",
      "2017-03-06 18:29:48,015 [INFO] Query 25:  8.016409 sec\n",
      "2017-03-06 18:29:55,675 [INFO] Query 26:  7.626479 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-062bfce25d9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mqueries_dataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mTimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mbenchmark\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatabase\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' benchmark time: %.07f sec'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-2e629fc2aa5d>\u001b[0m in \u001b[0;36mbenchmark\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mdataframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                 \u001b[0mquery_row\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rows'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Query \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_row\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'query_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m':  {:f} sec'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mquery_row\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32m/opt/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 883\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    884\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m   1026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1028\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "csv_filepath = 'tempresults.csv'\n",
    "rows = 10  # maximum number of rows to return from each query execution.\n",
    "database = 'Spark SQL'\n",
    "queries_filepath = 'queries/queries.csv'\n",
    "queries_dataframe = pandas.read_csv(queries_filepath)  # load queries from CSV file\n",
    "queries_dataframe = queries_dataframe[\n",
    "    queries_dataframe['database'] == database]  # filter queries on database name\n",
    "if not queries_dataframe.empty:\n",
    "    with Timer() as t:\n",
    "        benchmark()\n",
    "    logging.info(database + ' benchmark time: %.07f sec' % t.interval)\n",
    "else:\n",
    "    logging.warning(\"Missing \" + database + \" queries from \" + queries_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
